{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 说明\n",
    "项目主要是对BPE算法原理，进行了实现，实际中的库会更加复杂。       \n",
    "参考代码链接为：https://github.com/owenliang/bpe-tokenizer  \n",
    "(jupyter nbconvert --to markdown bpe.ipynb --output readme.md)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件\n",
    "cn=open(\"/home/minimind/xiaobo/dataset/train-cn.txt\",mode=\"r\",encoding=\"utf-8\").read()\n",
    "en=open(\"/home/minimind/xiaobo/dataset/train-en.txt\",mode=\"r\",encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777269,185768,963037\n"
     ]
    }
   ],
   "source": [
    "# 格式化数据 --字节列表 list(byte)\n",
    "doc_byte_list=[]\n",
    "cn_byte=[bytes([b]) for b in cn.encode(\"utf-8\")]\n",
    "en_byte=[bytes([b]) for b in en.encode(\"utf-8\")]\n",
    "doc_byte_list.extend(cn_byte)\n",
    "doc_byte_list.extend(en_byte)\n",
    "print(f\"{len(cn_byte)},{len(en_byte)},{len(doc_byte_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "class BPE():\n",
    "    def __init__(self):\n",
    "        self.b2t={} # 字节转tokens\n",
    "        self.t2b={} # tokens转字节\n",
    "        self.lenToken=0 # tokens字典大小\n",
    "    def _parse_states(self,arg_docByte):\n",
    "        stats={}\n",
    "        for i in range(len(arg_docByte)-1):\n",
    "            new_tokens=arg_docByte[i]+arg_docByte[i+1]\n",
    "            if new_tokens not in stats:\n",
    "                stats[new_tokens]=1\n",
    "            else:\n",
    "                stats[new_tokens]+=1\n",
    "        return stats\n",
    "    def __megar_pair(self,arg_docByte,arg_token):\n",
    "        megarTokens=[]\n",
    "        i=0\n",
    "        while i<len(arg_docByte):\n",
    "            if i+1 < len(arg_docByte) and arg_docByte[i]+arg_docByte[i+1]==arg_token:\n",
    "                megarTokens.append(arg_token)\n",
    "                i+=2\n",
    "            else:\n",
    "                megarTokens.append(arg_docByte[i])\n",
    "                i+=1\n",
    "        return megarTokens\n",
    "    \n",
    "    def train(self,arg_docByte,arg_vocalSize):\n",
    "        #单字节，基础token\n",
    "        for i in range(256):\n",
    "            self.b2t[bytes([i])]=i\n",
    "        self.lenToken=len(self.b2t)\n",
    "        processBar=tqdm(total=arg_vocalSize-self.lenToken)\n",
    "        while True:\n",
    "            if len(self.b2t) > arg_vocalSize:\n",
    "                break\n",
    "            #统计相邻token频率\n",
    "            states=self._parse_states(arg_docByte)\n",
    "            if len(states)==0:\n",
    "                break\n",
    "            #查找出现次数最多的组合\n",
    "            newToken=max(states,key=states.get)\n",
    "            arg_docByte=self.__megar_pair(arg_docByte,newToken)\n",
    "            self.b2t[newToken]=len(self.b2t)\n",
    "            processBar.update(1)\n",
    "        self.t2b={v:k for k ,v in self.b2t.items()}\n",
    "    def save(self,arg_fileName):\n",
    "        with open(arg_fileName,'wb') as fp:\n",
    "            fp.write(pickle.dumps((self.b2t,self.t2b)))\n",
    "    def load(self,arg_fileName):\n",
    "        with open(arg_fileName,'rb') as f : \n",
    "            self.b2t,self.t2b=pickle.loads(f.read())\n",
    "    def encode(self,text):\n",
    "        enc_tokens=[]\n",
    "        enc_docByte=[]\n",
    "        docByte=[bytes([b]) for b in text.encode(\"utf-8\")]\n",
    "        while True:\n",
    "            #合并相邻token\n",
    "            states = self._parse_states(docByte)\n",
    "            newToken=None\n",
    "            # 选择合并后id最小的pair合并（也就是优先合并短的）\n",
    "            for merga_token in states:\n",
    "                if merga_token in self.b2t and (newToken is None or self.b2t[merga_token]<self.b2t[newToken]):\n",
    "                    newToken=merga_token\n",
    "            if newToken ==None:\n",
    "                break\n",
    "            docByte=self.__megar_pair(docByte,newToken)\n",
    "        enc_tokens.extend([self.b2t[tok] for tok in docByte])\n",
    "        enc_docByte.extend(docByte)\n",
    "        return enc_tokens,enc_docByte\n",
    "    def decode(self,arg_tokens):\n",
    "        byteList=[]\n",
    "        for token in arg_tokens:\n",
    "            byteList.append(self.t2b[token])\n",
    "        return b''.join(byteList).decode(\"utf-8\",errors=\"replace\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/244 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [01:40,  2.44it/s]                         \n"
     ]
    }
   ],
   "source": [
    "bpe=BPE()\n",
    "bpe.train(doc_byte_list,500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe.save(\"test_bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe=BPE()\n",
    "bpe.load(\"test_bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 455, 32, 119, 111, 114, 108, 100]\n",
      "[b'h', b'e', b'll', b' ', b'w', b'o', b'r', b'l', b'd']\n"
     ]
    }
   ],
   "source": [
    "text=\"hello world\"\n",
    "token,byteInfo=bpe.encode(text)\n",
    "print(token)\n",
    "print(byteInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hell world\n"
     ]
    }
   ],
   "source": [
    "tx=bpe.decode(token)\n",
    "print(tx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
